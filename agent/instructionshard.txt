You have to run an experiment involving testing 3 Large Language Models (LLMs) on the MMLU dataset.\n
First, create a subset of 20 random questions from the Massive Multitask Language Understanding (MMLU) benchmark dataset present in /scratch/ss14247/worktest/mmlu/all/test-00000-of-00001.parquet. Appropriately analyse and understand the formatting of the file before conducting your analysis.\n
Test the 3 available LLMs one at a time, on this new subset of questions. The LLMs are labelled as LLM1, LLM2 and LLM3 and can be called by issuing a bash command in the format: python3 /scratch/ss14247/worktest/{LLM name}.py "{input prompt}"\n
Note that the LLMs perform best when run on one question at a time and you shouldn't provide the correct choice in the prompt. Additionally, the LLMs expect the options to be lettered such as A,B, or C and their output is from A,B,C and D. Change the format and structure of the subset to a CSV\n
Then for each LLM calculate it's accuracy on the task.
Report which LLM obtains the best accuracy on this MMLU subset and store the record in the submission file.

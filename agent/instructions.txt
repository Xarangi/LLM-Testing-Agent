You have to run an experiment involving testing 3 Large Language Models (LLMs) on the MMLU dataset.\n
First, create a subset of 20 random questions from the Massive Multitask Language Understanding (MMLU) benchmark dataset present in /scratch/ss14247/worktest/mmlu/all/test-00000-of-00001.parquet. Appropriately analyse and understand the formatting of the file before conducting your analysis.\n
Test the 3 available LLMs one at a time, on this new subset of questions. The LLMs are labelled as LLM1, LLM2 and LLM3 and can be called by issuing a bash command in the format: python3 /scratch/ss14247/worktest/{LLM name}.py "{input prompt}"\n
Note that the LLMs perform best when provided one question at a time and also need to be provided the choices so they can pick one. You shouldn't provide the answer in the prompt. The LLMs expect the options to be labeled A,B,C or D. Ensure the options passed to the LLMs have the proper labels attached before them. Additionally ensure the labels (after converting 0,1,2,3 to A,B,C,D) for the correct answers are stored for the accuracy calculation later. Appropriately write the required scripts to evaluate the subset for each LLM. Keep track of LLM responses for verification later.\n
Then for each LLM calculate it's accuracy on the task and ensure that the accuracies are correctly calculated.\n
Report which LLM obtains the best accuracy on this MMLU subset and store the record in the submission file.

You have to run an experiment involving testing 3 Large Language Models (LLMs) on the MMLU dataset.\n
First, create a subset of 20 random questions from the Massive Multitask Language Understanding (MMLU) benchmark dataset present in /scratch/ss14247/worktest/mmlu \n
Test 3 LLMs on this subset of questions and calculate their accuracy to provide the best performing LLM on the task. The names of the LLMs along with how to evaluate them using an API is given in scripts/llms.txt.\n
